{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1428a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chira\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "ats_snippets = [\n",
    "    \"Use action verbs like 'Led', 'Managed', 'Developed', instead of passive phrases.\",\n",
    "    \"Quantify your achievements, e.g., 'increased sales by 20%'.\",\n",
    "    \"Keep resume length to one page unless you have 10+ years of experience.\",\n",
    "    \"Tailor your resume to each job description by including relevant keywords.\",\n",
    "    \"Use consistent formatting: bullet points, font size, spacing.\",\n",
    "    \"Avoid vague terms like 'team player', focus on specific results.\",\n",
    "    \"List technical skills and tools separately in a skills section.\",\n",
    "    \"Start each bullet point with a powerful verb.\"\n",
    "]\n",
    "\n",
    "\n",
    "cv_guide_text = \"\"\"\n",
    "Every graduate student needs a curriculum vitae, or CV   \n",
    "Your CV represents your accomplishments and experience as an academic and helps to establish your \n",
    "professional image.  Well before you apply for faculty positions, you will use your CV to apply for \n",
    "fellowships and grants, to accompany submissions for publications or conference papers, when being \n",
    "considered for leadership roles or consulting projects, and more.  CV's are also used when applying for \n",
    "some positions outside academia, such as in think tanks or research institutes, or for research positions in \n",
    "industry. \n",
    "As you progress through graduate school, you will, of course, add to your CV, but the basic areas to \n",
    "include are your contact information, education, research experience, teaching experience, publications, \n",
    "presentations, honors and awards, and contact information for your references, or those people willing to \n",
    "speak or write on your behalf.   \n",
    "Some formatting pointers: \n",
    " There is no single best format. Refer to samples for ideas, but craft your CV to best reflect you\n",
    " and your unique accomplishments.\n",
    "  Unlike a resume, there is no page limit, but most graduate student's CVs are two to five pages in\n",
    " length.  Your CV may get no more than thirty seconds of the reader's attention, so ensure the\n",
    " most important information stands out. Keep it concise and relevant!\n",
    "  Be strategic in how you order and entitle your categories.  The most important information\n",
    " should be on the first page.  Within each category, list items in reverse chronological order.\n",
    " Category headings influence how readers perceive you. For example, the same experience could\n",
    " belong in a category entitled: “Service to the Field,” “Conferences Organized,” or “Relevant\n",
    " Professional Experience.”\n",
    "  Use active verbs and sentence fragments (not full sentences) to describe your experiences. Avoid\n",
    " pronouns (e.g. I, me), and minimize articles (a, and, the). Use a level of jargon most appropriate\n",
    " for your audience. Keep locations, dates and less important information on the right side of the\n",
    " page the left side should have important details like university, degree, job title, etc.\n",
    "  Stick to a common font, such as Times New Roman, using a font size of 10 to 12 point. Use\n",
    " highlighting judiciously, favoring bold, ALL CAPS, and white space to create a crisp\n",
    " professional style.  Avoid text boxes, underlining, and shading; italics may be used in\n",
    " moderation. Margins should be equal on all four sides, and be ¾ to 1 inch in size.\n",
    "  And most importantly…Follow the conventions of your field!  Different academic disciplines\n",
    " have different standards and expectations, especially in the order of categories.  Check out CVs\n",
    " from recent graduates of your department, and others in your field, to ensure you are following\n",
    " your field's norms.\n",
    " Tailor your CV to the position, purpose, or audience \n",
    "“Why should we select YOU?” - That is the question on the top of your reader's mind, so craft your CV \n",
    "to convince the reader that you have the skills, experience, and knowledge they seek. Depending on the \n",
    "purpose, you might place more or less emphasis on your teaching experience, for example. Also, keep \n",
    "an archival CV (for your eyes only!) that lists all the details of everything you've done - tailor from \n",
    "there.\n",
    "\"\"\"\n",
    "\n",
    "chunks = [chunk.strip() for chunk in re.split(r'\\n\\s*\\n', cv_guide_text) if chunk.strip()]\n",
    "\n",
    "embeddings = model.encode(chunks)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, \"cv_guide.index\")\n",
    "with open(\"cv_guide_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cdc469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_cv_guidelines(query_text, top_k=3):\n",
    "    query_embedding = model.encode([query_text]).astype(\"float32\")\n",
    "    index = faiss.read_index(\"cv_guide.index\")\n",
    "    with open(\"cv_guide_texts.pkl\", \"rb\") as f:\n",
    "        guide_chunks = pickle.load(f)\n",
    "\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [guide_chunks[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba6b16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_json = {\n",
    "  \"data\": {\n",
    "    \"classification\": {\n",
    "      \"contactInfo\": {\n",
    "        \"name\": \"Chiranjan Sahu\",\n",
    "        \"email\": \"chiranjancs01032006@gmail.com\",\n",
    "        \"phone\": \"9511897567\",\n",
    "        \"address\": \"B 301, Green Tower, Gilbert Hill Road, Andheri - 400058\",\n",
    "        \"linkedin\": \"https://www.linkedin.com/in/chiranjan-sahu-04342a28b/\"\n",
    "      },\n",
    "      \"education\": [\n",
    "        \"Bhartiya Vidya Bhavan’s Sardar Patel Institute of Technology (09/2023 - 08/2027), BTech - Computer Science and Engineering\",\n",
    "        \"Sonopant Dandekar Shikshan Mandali, Maharashtra Board, HSC (2023) - 80%\",\n",
    "        \"Anand Ashram English High School, Maharashtra Board, SSC (2021) - 80%\"\n",
    "      ],\n",
    "      \"experience\": [\n",
    "        {\n",
    "          \"role\": \"Senior Correspondent\",\n",
    "          \"organization\": \"SPark, S.P.I.T.\",\n",
    "          \"duration\": \"Current\",\n",
    "          \"description\": \"Led a team of 70 students and organized the best edition of 'Open Mic', the college’s platform for student performances.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"Intern\",\n",
    "          \"organization\": \"Ridlan AI Foundation\",\n",
    "          \"duration\": \"Upcoming\",\n",
    "          \"description\": \"Interning with Ridlan AI Foundation to work on AI-based solutions and contribute to research and development.\"\n",
    "        }\n",
    "      ],\n",
    "      \"projects\": [\n",
    "        {\n",
    "          \"name\": \"PDF QnA App\",\n",
    "          \"description\": \"Built an app to answer questions based on uploaded PDFs using prompt engineering and implemented RAG.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Credit Risk Analysis Model\",\n",
    "          \"description\": \"Developed a model to assess loan default probability based on financial history and personal attributes.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Attrition Prediction Model\",\n",
    "          \"description\": \"Trained multiple classification models with balanced data, achieving 95% accuracy without fine-tuning.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Credit Card Fraud Detection Model\",\n",
    "          \"description\": \"Used techniques like imbalanced learning, pipelining, and grid search to exceed 90% accuracy.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"NotesApp\",\n",
    "          \"description\": \"A terminal-based Python app connected to MySQL to store notes using the mysql library.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Reminders App\",\n",
    "          \"description\": \"A terminal-based app in Python using plyer and datetime for setting and storing reminders in JSON format.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"JobMatchr\",\n",
    "          \"description\": \"A resume enhancer that matches resumes with job openings, generates tailored cover letters, and outputs LaTeX resumes using RAG, Llama-3b via Groq, SerpAPI, and Jinja2 templating.\"\n",
    "        }\n",
    "      ],\n",
    "      \"skills\": [\n",
    "        \"C\",\n",
    "        \"C++\",\n",
    "        \"Python\",\n",
    "        \"Java\",\n",
    "        \"PyTorch\",\n",
    "        \"TensorFlow\",\n",
    "        \"Scikit-learn\",\n",
    "        \"NumPy\",\n",
    "        \"Pandas\",\n",
    "        \"Huggingface\",\n",
    "        \"OpenAI\",\n",
    "        \"Groq\",\n",
    "        \"Langchain\",\n",
    "        \"Langflow\",\n",
    "        \"Phidata\",\n",
    "        \"LLMs\",\n",
    "        \"GenAI\",\n",
    "        \"Agentic AI\"\n",
    "      ],\n",
    "      \"certifications\": [],\n",
    "      \"achievements\": [\n",
    "        \"2x Best Team Award - Abhimat-Lok Sabha Debate winner (2024, 2025)\",\n",
    "        \"3rd Place - Avalanche National Crisis Management Case Study (E-Cell S.P.I.T.)\",\n",
    "        \"2nd Place - VisionX Case Study (N.I.S.P.-S.P.I.T.)\",\n",
    "        \"Finalist - MLtiverse-Fidlatica, Top 10 out of 250+ (SCMHRD)\",\n",
    "        \"Finalist - INSPIRIOBIZ 5.0, Top 10 out of 270+ (VESIM)\",\n",
    "        \"Overall 3rd place in SE Hackathon - S.P.I.T.\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c8ff5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_resume_json(resume_json):\n",
    "    classification = resume_json[\"data\"][\"classification\"]\n",
    "    parts = []\n",
    "\n",
    "    # Contact Info\n",
    "    contact = classification.get(\"contactInfo\", {})\n",
    "    parts.append(f\"Name: {contact.get('name', '')}\")\n",
    "    parts.append(f\"Email: {contact.get('email', '')}\")\n",
    "    parts.append(f\"Phone: {contact.get('phone', '')}\")\n",
    "    parts.append(f\"Address: {contact.get('address', '')}\")\n",
    "    parts.append(f\"LinkedIn: {contact.get('linkedin', '')}\")\n",
    "\n",
    "    # Education\n",
    "    education = classification.get(\"education\", [])\n",
    "    if education:\n",
    "        parts.append(\"\\nEducation:\")\n",
    "        for edu in education:\n",
    "            parts.append(f\"- {edu}\")\n",
    "\n",
    "    # Experience\n",
    "    experience = classification.get(\"experience\", [])\n",
    "    if experience:\n",
    "        parts.append(\"\\nExperience:\")\n",
    "        for exp in experience:\n",
    "            parts.append(f\"- {exp['role']} at {exp['organization']} ({exp['duration']}): {exp['description']}\")\n",
    "\n",
    "    # Projects\n",
    "    projects = classification.get(\"projects\", [])\n",
    "    if projects:\n",
    "        parts.append(\"\\nProjects:\")\n",
    "        for proj in projects:\n",
    "            parts.append(f\"- {proj['name']}: {proj['description']}\")\n",
    "\n",
    "    # Skills\n",
    "    skills = classification.get(\"skills\", [])\n",
    "    if skills:\n",
    "        parts.append(\"\\nSkills: \" + \", \".join(skills))\n",
    "\n",
    "    # Certifications\n",
    "    certs = classification.get(\"certifications\", [])\n",
    "    if certs:\n",
    "        parts.append(\"\\nCertifications:\")\n",
    "        for cert in certs:\n",
    "            parts.append(f\"- {cert}\")\n",
    "\n",
    "    # Achievements\n",
    "    achievements = classification.get(\"achievements\", [])\n",
    "    if achievements:\n",
    "        parts.append(\"\\nAchievements:\")\n",
    "        for ach in achievements:\n",
    "            parts.append(f\"- {ach}\")\n",
    "\n",
    "    return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73ad61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embed_resume_for_future_matching(resume_text):\n",
    "    emb = model.encode([resume_text]).astype(\"float32\")\n",
    "    index = faiss.IndexFlatL2(emb.shape[1])\n",
    "    index.add(emb)\n",
    "    faiss.write_index(index, \"resume_vectors.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "701c32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = flatten_resume_json(resume_json)\n",
    "rag_context = retrieve_cv_guidelines(resume_text, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e5c81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import requests\n",
    "\n",
    "def build_prompt(resume_text, rag_context):\n",
    "    return f\"\"\"\n",
    "You are a resume enhancement AI.\n",
    "\n",
    "From the following raw resume data and RAG context, extract and rewrite content into structured professional resume sections: About, Skills, Experience, Education, Projects, Certifications, and Achievements.\n",
    "\n",
    "Only return the enhanced resume content. Content should fit into a single page. Do NOT include any explanations, notes, or repeat the prompt.\n",
    "=== RAG CONTEXT ===\n",
    "{rag_context}\n",
    "=== Resume Input ===\n",
    "{resume_text}\n",
    "\n",
    "=== Enhanced Resume ===\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9612ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "def query_groq(prompt, model=\"llama3-8b-8192\"):\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a resume enhancer AI. Output structured resume sections only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7718e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(resume_text, rag_context)\n",
    "enhanced_resume = query_groq(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d4f4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_enhanced_resume(raw_text):\n",
    "    import re\n",
    "\n",
    "    section_titles = [\n",
    "        \"About\", \"Skills\", \"Experience\", \"Education\", \"Projects\",\n",
    "        \"Certifications\", \"Achievements\"\n",
    "    ]\n",
    "    \n",
    "    # Define a clean dict to hold content\n",
    "    sections = {title.lower(): [] for title in section_titles}\n",
    "    current_section = None\n",
    "\n",
    "    # Clean unwanted symbols from a line\n",
    "    def clean_line(line):\n",
    "        return re.sub(r\"^[\\s•*+\\-]+\", \"\", line).strip()\n",
    "\n",
    "    lines = raw_text.strip().splitlines()\n",
    "    for line in lines:\n",
    "        match = re.match(r\"\\*\\*(.*?)\\*\\*\", line.strip())\n",
    "        if match:\n",
    "            header = match.group(1).strip()\n",
    "            if header in section_titles:\n",
    "                current_section = header.lower()\n",
    "                continue\n",
    "\n",
    "        if current_section:\n",
    "            content = clean_line(line)\n",
    "            if content:\n",
    "                sections[current_section].append(content)\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "metadata = resume_json[\"data\"][\"classification\"][\"contactInfo\"]\n",
    "parsed_data = parse_enhanced_resume(enhanced_resume)\n",
    "\n",
    "resume_data = {\n",
    "    \"name\": metadata.get(\"name\", \"\"),\n",
    "    \"email\": metadata.get(\"email\", \"\"),\n",
    "    \"phone\": metadata.get(\"phone\", \"\"),\n",
    "    \"address\": metadata.get(\"address\", \"\"),\n",
    "    \"linkedin\": metadata.get(\"linkedin\", \"\"),\n",
    "    \"about\": \" \".join(parsed_data.get(\"about\", [])) or \"N/A\",\n",
    "    \"skills\": parsed_data.get(\"skills\", []),\n",
    "    \"experience\": parsed_data.get(\"experience\", []),\n",
    "    \"education\": parsed_data.get(\"education\", []),\n",
    "    \"projects\": parsed_data.get(\"projects\", []),\n",
    "    \"certifications\": parsed_data.get(\"certifications\", []),\n",
    "    \"achievements\": parsed_data.get(\"achievements\", []),\n",
    "}\n",
    "\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "def render_latex(resume_data, template_path=\"resume_template.tex\"):\n",
    "    env = Environment(loader=FileSystemLoader('.'))\n",
    "    template = env.get_template(template_path)\n",
    "    return template.render(resume_data)\n",
    "\n",
    "latex_code = render_latex(resume_data)\n",
    "\n",
    "with open(\"resume_output.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f89faeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import serpapi\n",
    "import json\n",
    "def get_multiple_jobs_with_pagination(job_title, location):\n",
    "    params = {\n",
    "        \"engine\": \"google_jobs\",\n",
    "        \"q\": job_title,\n",
    "        \"location\": location,\n",
    "        \"api_key\": '83c1ef3c99b32b05ab29da61937948e1cce626b355feb3c4c6ead197a08a7aac',\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"in\"\n",
    "    }\n",
    "    max_jobs = 5\n",
    "    all_jobs = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(all_jobs) < max_jobs:\n",
    "        if next_page_token:\n",
    "            params[\"next_page_token\"] = next_page_token\n",
    "        else:\n",
    "            params.pop(\"next_page_token\", None)\n",
    "\n",
    "        search = serpapi.search(params)   # returns SerpResults (dict-like)\n",
    "        data = search          \n",
    "\n",
    "        jobs = data.get(\"jobs_results\", [])\n",
    "        all_jobs.extend(jobs)\n",
    "\n",
    "        # Pagination\n",
    "        serpapi_pagination = data.get(\"serpapi_pagination\", {})\n",
    "        next_page_token = serpapi_pagination.get(\"next_page_token\")\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    all_jobs = all_jobs[:max_jobs]\n",
    "\n",
    "    result = {}\n",
    "    for idx, job in enumerate(all_jobs, 1):\n",
    "        description = job.get('description', '')\n",
    "        company_name = job.get('company_name', '')\n",
    "        application_link = \"\"\n",
    "        if 'apply_options' in job and job['apply_options']:\n",
    "            application_link = job['apply_options'][0].get('link', '')\n",
    "        elif 'via' in job:\n",
    "            application_link = job['via']\n",
    "        else:\n",
    "            application_link = job.get('detected_extensions', {}).get('apply_link', '')\n",
    "        \n",
    "        actual_job_title = job.get('title', f\"{job_title} Opportunity {idx}\")\n",
    "        result[actual_job_title] = {\n",
    "            \"company_name\": company_name,\n",
    "            \"description\": description,\n",
    "            \"application_link\": application_link\n",
    "        }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5785a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = \"Python Developer\"\n",
    "location = \"India\"\n",
    "job_descriptions_json = get_multiple_jobs_with_pagination(job_title, location)\n",
    "\n",
    "# Step 2: Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 3: Extract descriptions and metadata\n",
    "descriptions = []\n",
    "metadata = []\n",
    "\n",
    "for title, data in job_descriptions_json.items():\n",
    "    descriptions.append(data[\"description\"])\n",
    "    metadata.append({\n",
    "        \"title\": title,\n",
    "        \"company_name\": data[\"company_name\"],\n",
    "        \"application_link\": data[\"application_link\"]\n",
    "    })\n",
    "\n",
    "# Step 4: Generate embeddings\n",
    "embeddings = model.encode(descriptions)\n",
    "embeddings_np = np.array(embeddings).astype(\"float32\")  # FAISS requires float32\n",
    "\n",
    "# Step 5: Create FAISS index and add embeddings\n",
    "dimension = embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Optional: Save FAISS index\n",
    "faiss.write_index(index, \"job_faiss.index\")\n",
    "\n",
    "# Step 6: Save metadata for lookup\n",
    "with open(\"job_faiss_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "134f8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"job_faiss.index\")\n",
    "with open(\"job_faiss_metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cd98044",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_resume_text = enhanced_resume\n",
    "resume_embedding = model.encode([user_resume_text]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0a4b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Matching Jobs:\n",
      "\n",
      "Python Developer- Banking at JP Techno Park\n",
      "Link: https://in.indeed.com/viewjob?jk=ce7d34796d29df40&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
      "\n",
      "Python Developer with AI Experience at Infugin Technologies\n",
      "Link: https://in.linkedin.com/jobs/view/python-developer-with-ai-experience-at-infugin-technologies-4212626026?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
      "\n",
      "Java Scala + Python Developer at Tata Consultancy Services (Tcs)\n",
      "Link: https://apna.co/job/mumbai/java-scala-python-1858767732?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "D, I = index.search(resume_embedding, top_k)\n",
    "\n",
    "print(\"Top Matching Jobs:\\n\")\n",
    "for idx in I[0]:\n",
    "    job = metadata[idx]\n",
    "    print(f\"{job['title']} at {job['company_name']}\\nLink: {job['application_link']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44330c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "def query_groq2(prompt, model=\"llama3-8b-8192\"):\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a resume enhancer AI. Output structured resume sections only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8db19079",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc_text = \"\\n\\n\".join([metadata[i][\"title\"] + \":\\n\" + descriptions[i] for i in I[0]])\n",
    "\n",
    "rag_prompt = f\"\"\"\n",
    "You are a career advisor AI. The following is a candidate's resume:\n",
    "\n",
    "--- RESUME ---\n",
    "{user_resume_text}\n",
    "\n",
    "These are the job descriptions of top matches:\n",
    "\n",
    "--- JOB DESCRIPTIONS ---\n",
    "{job_desc_text}\n",
    "\n",
    "1. Identify what technical or domain-specific skills the candidate is missing.\n",
    "2. Recommend a step-by-step learning path (with topics/tools/technologies) to bridge the gap.\n",
    "3. Suggest resources (platforms or certifications) for each skill if possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bb2a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_path = query_groq2(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1771dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "def query_groq3(prompt, model=\"llama3-8b-8192\"):\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a resume enhancer AI. Output structured resume sections only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "707058fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cover_letter(user_resume, selected_job_title, selected_job_description, company_name):\n",
    "    prompt = f\"\"\"\n",
    "Write a personalized and professional cover letter for the position of \"{selected_job_title}\" at {company_name}.\n",
    "The letter should be 3-4 paragraphs, tailored to the job description below, and should highlight how the candidate's skills align with the company's requirements.\n",
    "\n",
    "--- Candidate's Resume ---\n",
    "{user_resume}\n",
    "\n",
    "--- Job Description ---\n",
    "{selected_job_description}\n",
    "\n",
    "Ensure the tone is confident, enthusiastic, and formal. Avoid generic phrases. Mention specific skills or experiences from the resume that match the job. End with a call to action and interest in an interview.\n",
    "Begin your response directly from the actual response, no need to give headers like 'Here is your generated cover letter'.\n",
    "\"\"\"\n",
    "\n",
    "    return query_groq3(prompt, model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7f389f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cover letter to a .txt file\n",
    "def save_cover_letter_to_txt(cover_letter_text, filename=\"cover_letter.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cover_letter_text)\n",
    "    print(f\"Cover letter saved as '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "778fd2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover letter saved as 'cover_letter.txt'\n"
     ]
    }
   ],
   "source": [
    "# Let's say the user selects job index 1 (second from search results)\n",
    "selected_index = I[0][1]\n",
    "selected_job = metadata[selected_index]\n",
    "\n",
    "selected_title = selected_job.get(\"title\", \"Python Developer\")  # fallback\n",
    "selected_description = descriptions[selected_index]\n",
    "selected_company = selected_job[\"company_name\"]\n",
    "\n",
    "cover_letter = generate_cover_letter(\n",
    "    user_resume=user_resume_text,\n",
    "    selected_job_title=selected_title,\n",
    "    selected_job_description=selected_description,\n",
    "    company_name=selected_company\n",
    ")\n",
    "\n",
    "save_cover_letter_to_txt(cover_letter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
