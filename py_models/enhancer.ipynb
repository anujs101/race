{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a45fc613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import time\n",
    "import serpapi\n",
    "import json\n",
    "import os\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1428a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "ats_snippets = [\n",
    "    \"Use action verbs like 'Led', 'Managed', 'Developed', instead of passive phrases.\",\n",
    "    \"Quantify your achievements, e.g., 'increased sales by 20%'.\",\n",
    "    \"Keep resume length to one page unless you have 10+ years of experience.\",\n",
    "    \"Tailor your resume to each job description by including relevant keywords.\",\n",
    "    \"Use consistent formatting: bullet points, font size, spacing.\",\n",
    "    \"Avoid vague terms like 'team player', focus on specific results.\",\n",
    "    \"List technical skills and tools separately in a skills section.\",\n",
    "    \"Start each bullet point with a powerful verb.\"\n",
    "]\n",
    "\n",
    "chunks = [chunk.strip() for chunk in ats_snippets if chunk.strip()]\n",
    "\n",
    "embeddings = model.encode(chunks)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, \"cv_guide.index\")\n",
    "with open(\"cv_guide_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4cdc469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_cv_guidelines(query_text, top_k=3):\n",
    "    query_embedding = model.encode([query_text]).astype(\"float32\")\n",
    "    index = faiss.read_index(\"cv_guide.index\")\n",
    "    with open(\"cv_guide_texts.pkl\", \"rb\") as f:\n",
    "        guide_chunks = pickle.load(f)\n",
    "\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [guide_chunks[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_json = {\n",
    "    \"data\": {\n",
    "        \"classification\": {\n",
    "            \"contactInfo\": {\n",
    "                \"name\": \"Anurag Pitabasa Sahu\",\n",
    "                \"email\": \"Anurag.S25PGAI@jioinstitute.edu.in\",\n",
    "                \"phone\": \"9370815205\",\n",
    "                \"address\": \"Mumbai\",  # Address wasn't directly mentioned, assuming location of Jio Institute.\n",
    "                \"linkedin\": \"https://www.linkedin.com/in/anurag-p-sahu/\"  # Not given in the PDF\n",
    "            },\n",
    "            \"education\": [\n",
    "                \"Post Graduate Program in AI & Data Science, Jio Institute, 2025 (CGPA: 6.71)\",\n",
    "                \"Bachelor of Technology - Bioinformatics, Vignan Foundation for Science Technology and Research (Vignan University), 2024 (CGPA: 8.23)\",\n",
    "                \"Class XII, Sardar Vallabhbhai Patel Jr. College of Science, MSBSHSE, 2020 (65.85%)\",\n",
    "                \"Class X, Anand Ashram English High School, MSBSHSE, 2018 (83.4%)\"\n",
    "            ],\n",
    "            \"experience\": [\n",
    "                {\n",
    "                    \"role\": \"Data Science and Analytics Intern\",\n",
    "                    \"organization\": \"Hardcastle Restaurant Pvt Ltd (McDonald's India)\",\n",
    "                    \"duration\": \"Oct 2024 - Dec 2024\",\n",
    "                    \"description\": \"Performed EDA on delivery data, set operational targets, classified store performance, and built a predictive model using XGBoost Regressor achieving RÂ² of 0.98.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"Software Engineering Intern\",\n",
    "                    \"organization\": \"New Era IT Consultancy\",\n",
    "                    \"duration\": \"Dec 2023 - Apr 2024\",\n",
    "                    \"description\": \"Worked on HTML/CSS frontend development, Git version control, SQL database management, and built deep learning-based gesture recognition system with 80% accuracy.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"Machine Learning Intern\",\n",
    "                    \"organization\": \"National Institute of Technology Agartala\",\n",
    "                    \"duration\": \"Jan 2023 - Apr 2023\",\n",
    "                    \"description\": \"Performed statistical analysis for stress classification and optimized resource allocation for store performance during high-stress periods.\"\n",
    "                }\n",
    "            ],\n",
    "            \"projects\": [\n",
    "                {\n",
    "                    \"name\": \"AI Drug Agent: Predicting Drug Permeability\",\n",
    "                    \"description\": \"Developing a multi-agent AI system using Random Forest models to predict drug permeability and optimize drug discovery pipelines.\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Email Security: End-to-End MLOps Spam Classifier\",\n",
    "                    \"description\": \"Built a real-time spam detection system with Naive Bayes, TF-IDF, Flask REST API, and MLOps pipeline achieving 98% accuracy.\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Plant Disease Detection Using CNNs and Transfer Learning\",\n",
    "                    \"description\": \"Built a multi-class classification system using pretrained ResNet50 for plant disease detection, achieving 96% accuracy.\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Web Data ETL Pipeline for Text Analysis\",\n",
    "                    \"description\": \"Developed a web scraping and NLP-based ETL pipeline using Python, BeautifulSoup, and NLTK for structured text analysis.\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Customer Segmentation using RFM Analysis\",\n",
    "                    \"description\": \"Performed customer segmentation using K-Means clustering with Python libraries (Pandas, Matplotlib) to improve targeted marketing strategies.\"\n",
    "                }\n",
    "            ],\n",
    "            \"skills\": [\n",
    "                \"Power BI\",\n",
    "                \"Tableau\",\n",
    "                \"Python\",\n",
    "                \"MS Excel\",\n",
    "                \"Scikit-Learn\",\n",
    "                \"Pandas\",\n",
    "                \"SQL\",\n",
    "                \"Git\",\n",
    "                \"Postman\",\n",
    "                \"NumPy\",\n",
    "                \"Machine Learning\",\n",
    "                \"Deep Learning\",\n",
    "                \"Data Visualization\",\n",
    "                \"Probability and Statistics\",\n",
    "                \"REST API Development (Flask)\",\n",
    "                \"EDA (Exploratory Data Analysis)\",\n",
    "                \"NLP (Natural Language Processing)\"\n",
    "            ],\n",
    "            \"certifications\": [\n",
    "                \"PCAP: Programming Essentials in Python - OPENEDG Cisco Network Academy\",\n",
    "                \"IT Business Analyst Certification - BACentric Solutions (IIBA-EEP)\",\n",
    "                \"Blockchain Technology Short Course\",\n",
    "                \"Study Abroad Module - NTU Singapore\",\n",
    "                \"Industry Visit: Seagate Singapore Design Center and Global Fintech Institute\"\n",
    "            ],\n",
    "            \"achievements\": [\n",
    "                \"Research Paper published in Springer on Deep Learning Based Real-Time Hand Gesture Recognition\",\n",
    "                \"Best Engineer Award - Vignan University (Sep 2023)\"\n",
    "            ],\n",
    "            \"publications\": [\n",
    "                \"Deep Learning Based Real-Time Hand Gesture Recognition, Springer Journal, July 2023\"\n",
    "            ],\n",
    "            \"conferences\": [\n",
    "                \"Presented research paper at 5th International Conference in Computational Intelligence in Pattern Recognition, Techno Main Salt Lake, Kolkata, May 2023\"\n",
    "            ],\n",
    "            \"languages\": [\n",
    "                \"Hindi\",\n",
    "                \"Odia\",\n",
    "                \"Marathi\",\n",
    "                \"English\",\n",
    "                \"Telugu\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2c8ff5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_resume_json(resume_json):\n",
    "    classification = resume_json[\"data\"][\"classification\"]\n",
    "    parts = []\n",
    "\n",
    "    # Contact Info\n",
    "    contact = classification.get(\"contactInfo\", {})\n",
    "    parts.append(f\"Name: {contact.get('name', '')}\")\n",
    "    parts.append(f\"Email: {contact.get('email', '')}\")\n",
    "    parts.append(f\"Phone: {contact.get('phone', '')}\")\n",
    "    parts.append(f\"Address: {contact.get('address', '')}\")\n",
    "    parts.append(f\"LinkedIn: {contact.get('linkedin', '')}\")\n",
    "\n",
    "    # Education\n",
    "    education = classification.get(\"education\", [])\n",
    "    if education:\n",
    "        parts.append(\"\\nEducation:\")\n",
    "        for edu in education:\n",
    "            parts.append(f\"- {edu}\")\n",
    "\n",
    "    # Experience\n",
    "    experience = classification.get(\"experience\", [])\n",
    "    if experience:\n",
    "        parts.append(\"\\nExperience:\")\n",
    "        for exp in experience:\n",
    "            parts.append(f\"- {exp['role']} at {exp['organization']} ({exp['duration']}): {exp['description']}\")\n",
    "\n",
    "    # Projects\n",
    "    projects = classification.get(\"projects\", [])\n",
    "    if projects:\n",
    "        parts.append(\"\\nProjects:\")\n",
    "        for proj in projects:\n",
    "            parts.append(f\"- {proj['name']}: {proj['description']}\")\n",
    "\n",
    "    # Skills\n",
    "    skills = classification.get(\"skills\", [])\n",
    "    if skills:\n",
    "        parts.append(\"\\nSkills: \" + \", \".join(skills))\n",
    "\n",
    "    # Certifications\n",
    "    certs = classification.get(\"certifications\", [])\n",
    "    if certs:\n",
    "        parts.append(\"\\nCertifications:\")\n",
    "        for cert in certs:\n",
    "            parts.append(f\"- {cert}\")\n",
    "\n",
    "    # Achievements\n",
    "    achievements = classification.get(\"achievements\", [])\n",
    "    if achievements:\n",
    "        parts.append(\"\\nAchievements:\")\n",
    "        for ach in achievements:\n",
    "            parts.append(f\"- {ach}\")\n",
    "\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "73ad61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_resume_for_future_matching(resume_text):\n",
    "    emb = model.encode([resume_text]).astype(\"float32\")\n",
    "    index = faiss.IndexFlatL2(emb.shape[1])\n",
    "    index.add(emb)\n",
    "    faiss.write_index(index, \"resume_vectors.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0e5c81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(resume_json):\n",
    "    resume_text = flatten_resume_json(resume_json)\n",
    "    rag_context = retrieve_cv_guidelines(resume_text, top_k=3)\n",
    "    embed_resume_for_future_matching(resume_text)\n",
    "\n",
    "    return f\"\"\"\n",
    "        You are a resume enhancement AI.\n",
    "\n",
    "        From the following raw resume data and RAG context, extract and rewrite content into structured professional resume sections: About, Skills, Experience, Education, Projects, Certifications, and Achievements.\n",
    "\n",
    "        Only return the enhanced resume content. Content should fit into a single page. Do NOT include any explanations, notes, or repeat the prompt.\n",
    "        === RAG CONTEXT ===\n",
    "        {rag_context}\n",
    "        === Resume Input ===\n",
    "        {resume_text}\n",
    "\n",
    "        === Enhanced Resume ===\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9612ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "def query_groq(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful resume enhancement assistant that interprets user's resume and enhances them while matching their resumes with suitable jobs and suggesting ways to the user to upskill.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        stop=None,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eaf8f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_user_prompt(user_prompt):\n",
    "    \"\"\"\n",
    "    Takes raw user instruction and structures it nicely for LLM.\n",
    "    If user_prompt is empty, return empty string.\n",
    "    \"\"\"\n",
    "    if not user_prompt.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Otherwise structure it properly\n",
    "    return f\"\"\"\n",
    "    Additional Instructions for Modification:\n",
    "\n",
    "    {user_prompt}\n",
    "\n",
    "    Apply these modifications to the resume along with the enhancement.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "724de393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_resume(user_prompt=\"\", resume_json=None):\n",
    "    if resume_json is None:\n",
    "        raise ValueError(\"Resume JSON must be provided.\")\n",
    "\n",
    "    # Step 1: Structure user instruction\n",
    "    structured_instruction = structure_user_prompt(user_prompt)\n",
    "\n",
    "    # Step 2: Build the basic enhancement prompt\n",
    "    base_prompt = build_prompt(resume_json)\n",
    "\n",
    "    # Step 3: Combine prompts\n",
    "    final_prompt = base_prompt + structured_instruction\n",
    "\n",
    "    # Step 4: Send to Groq (or any LLM)\n",
    "    return query_groq(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7d4f4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_enhanced_resume(resume_json):\n",
    "    raw_text = modify_resume(resume_json=resume_json)\n",
    "\n",
    "    metadata = resume_json[\"data\"][\"classification\"][\"contactInfo\"]\n",
    "    section_titles = [\n",
    "        \"About\", \"Skills\", \"Experience\", \"Education\", \"Projects\",\n",
    "        \"Certifications\", \"Achievements\"\n",
    "    ]\n",
    "\n",
    "    sections = {title.lower(): [] for title in section_titles}\n",
    "    current_section = None\n",
    "\n",
    "    lines = raw_text.strip().splitlines()\n",
    "    for line in lines:\n",
    "        # Detect section headers like **Skills**\n",
    "        match = re.match(r\"\\*\\*(.*?)\\*\\*\", line.strip())\n",
    "        if match:\n",
    "            header = match.group(1).strip()\n",
    "            if header in section_titles:\n",
    "                current_section = header.lower()\n",
    "                continue\n",
    "\n",
    "        # Store content lines under the current section\n",
    "        if current_section:\n",
    "            content = line.strip(\"â¢\").strip(\"-\").strip()\n",
    "            if content:\n",
    "                sections[current_section].append(content)\n",
    "\n",
    "    # Now build the final resume JSON\n",
    "    def get_single_line(section_name):\n",
    "        items = sections.get(section_name.lower(), [])\n",
    "        return items[0] if items else \"\"\n",
    "\n",
    "    def clean_items(items):\n",
    "        if items is None:\n",
    "            return []\n",
    "        cleaned = []\n",
    "        for item in items:\n",
    "            item = item.lstrip('*+â¢- ').strip()\n",
    "            cleaned.append(item)\n",
    "        return cleaned\n",
    "\n",
    "    parsed_resume = {\n",
    "        \"name\": metadata.get(\"name\", \"\"),\n",
    "        \"email\": metadata.get(\"email\", \"\"),\n",
    "        \"phone\": metadata.get(\"phone\", \"\"),\n",
    "        \"address\": metadata.get(\"address\", \"\"),\n",
    "        \"linkedin\": metadata.get(\"linkedin\", \"\"),\n",
    "        \"about\": get_single_line(\"About\"),\n",
    "        \"skills\": clean_items(sections.get(\"skills\", [])),\n",
    "        \"experience\": clean_items(sections.get(\"experience\", [])),\n",
    "        \"education\": clean_items(sections.get(\"education\", [])),\n",
    "        \"projects\": clean_items(sections.get(\"projects\", [])),\n",
    "        \"certifications\": clean_items(sections.get(\"certifications\", [])),\n",
    "        \"achievements\": clean_items(sections.get(\"achievements\", []))\n",
    "    }\n",
    "\n",
    "    return parsed_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8a8fe291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_latex(resume_json, template_path=\"resume_template.tex\"):\n",
    "    resume_data = parse_enhanced_resume(resume_json)\n",
    "    \n",
    "    env = Environment(loader=FileSystemLoader('.'))\n",
    "    template = env.get_template(template_path)\n",
    "    latex_code =  template.render(resume_data)\n",
    "    with open(\"resume_output.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(latex_code)\n",
    "    return latex_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f89faeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_jobs_with_pagination(job_title, location):\n",
    "    params = {\n",
    "        \"engine\": \"google_jobs\",\n",
    "        \"q\": job_title,\n",
    "        \"location\": location,\n",
    "        \"api_key\": '83c1ef3c99b32b05ab29da61937948e1cce626b355feb3c4c6ead197a08a7aac',\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"in\"\n",
    "    }\n",
    "    max_jobs = 5\n",
    "    all_jobs = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(all_jobs) < max_jobs:\n",
    "        if next_page_token:\n",
    "            params[\"next_page_token\"] = next_page_token\n",
    "        else:\n",
    "            params.pop(\"next_page_token\", None)\n",
    "\n",
    "        search = serpapi.search(params)   # returns SerpResults (dict-like)\n",
    "        data = search          \n",
    "\n",
    "        jobs = data.get(\"jobs_results\", [])\n",
    "        all_jobs.extend(jobs)\n",
    "\n",
    "        # Pagination\n",
    "        serpapi_pagination = data.get(\"serpapi_pagination\", {})\n",
    "        next_page_token = serpapi_pagination.get(\"next_page_token\")\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    all_jobs = all_jobs[:max_jobs]\n",
    "\n",
    "    result = {}\n",
    "    for idx, job in enumerate(all_jobs, 1):\n",
    "        description = job.get('description', '')\n",
    "        company_name = job.get('company_name', '')\n",
    "        application_link = \"\"\n",
    "        if 'apply_options' in job and job['apply_options']:\n",
    "            application_link = job['apply_options'][0].get('link', '')\n",
    "        elif 'via' in job:\n",
    "            application_link = job['via']\n",
    "        else:\n",
    "            application_link = job.get('detected_extensions', {}).get('apply_link', '')\n",
    "        \n",
    "        actual_job_title = job.get('title', f\"{job_title} Opportunity {idx}\")\n",
    "        result[actual_job_title] = {\n",
    "            \"company_name\": company_name,\n",
    "            \"description\": description,\n",
    "            \"application_link\": application_link\n",
    "        }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5785a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = \"Python Developer\"\n",
    "location = \"India\"\n",
    "\n",
    "def embed_job_data(job_title, location):\n",
    "    job_descriptions_json = get_multiple_jobs_with_pagination(job_title, location)\n",
    "\n",
    "    descriptions = []\n",
    "    metadata = []\n",
    "\n",
    "    for title, data in job_descriptions_json.items():\n",
    "        description = data.get(\"description\", \"\")\n",
    "        descriptions.append(description)\n",
    "\n",
    "        metadata.append({\n",
    "            \"title\": title,\n",
    "            \"company_name\": data.get(\"company_name\", \"\"),\n",
    "            \"application_link\": data.get(\"application_link\", \"\"),\n",
    "            \"description\": description  # ð¥ also saving description inside metadata now\n",
    "        })\n",
    "\n",
    "    # Step 4: Generate embeddings\n",
    "    embeddings = model.encode(descriptions)\n",
    "    embeddings_np = np.array(embeddings).astype(\"float32\")  # FAISS requires float32\n",
    "\n",
    "    # Step 5: Create FAISS index and add embeddings\n",
    "    dimension = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_np)\n",
    "\n",
    "    # Optional: Save FAISS index\n",
    "    faiss.write_index(index, \"job_faiss.index\")\n",
    "\n",
    "    # Step 6: Save metadata (with descriptions) for lookup\n",
    "    with open(\"job_faiss_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5de59828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_jobs(resume_json):\n",
    "    enhanced_resume = parse_enhanced_resume(resume_json)\n",
    "    render_latex(resume_json)\n",
    "\n",
    "    index = faiss.read_index(\"job_faiss.index\")\n",
    "    \n",
    "    with open(\"job_faiss_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Ensure model is loaded\n",
    "    resume_embedding = model.encode([enhanced_resume]).astype(\"float32\")\n",
    "\n",
    "    top_k = 3\n",
    "    D, I = index.search(resume_embedding, top_k)\n",
    "\n",
    "    matched_jobs = []\n",
    "\n",
    "    for idx in I[0]:\n",
    "        job = metadata[idx]\n",
    "        matched_jobs.append({\n",
    "            \"title\": job.get('title', ''),\n",
    "            \"company_name\": job.get('company_name', ''),\n",
    "            \"application_link\": job.get('application_link', ''),\n",
    "            \"description\": job.get('description', '')  # ð¥ Include description now\n",
    "        })\n",
    "    \n",
    "    return json.dumps({\"matched_jobs\": matched_jobs}, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8db19079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_learning_path(resume_json) :\n",
    "    enhanced_resume = parse_enhanced_resume(resume_json)\n",
    "    job_desc = match_jobs(resume_json)\n",
    "    rag_prompt = f\"\"\"\n",
    "        You are a career advisor AI. The following is a candidate's resume:\n",
    "\n",
    "        --- RESUME ---\n",
    "        {enhanced_resume}\n",
    "\n",
    "        These are the job descriptions of top matches:\n",
    "\n",
    "        --- JOB DESCRIPTIONS ---\n",
    "        {job_desc}\n",
    "\n",
    "        1. Identify what technical or domain-specific skills the candidate is missing.\n",
    "        2. Recommend a step-by-step learning path (with topics/tools/technologies) to bridge the gap.\n",
    "        3. Suggest resources (platforms or certifications) for each skill if possible.\n",
    "    \"\"\"\n",
    "    \n",
    "    return query_groq(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0b54bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_path = generate_learning_path(resume_json = resume_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "90d88c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. Identification of Missing Technical or Domain-Specific Skills\n",
      "\n",
      "Based on the candidate's resume and the job descriptions provided, the following technical or domain-specific skills appear to be missing or could be enhanced:\n",
      "\n",
      "- **Experience with Azure and its ecosystem (Azure AD, Azure open AI models, AKS)**: While the candidate has experience with Python, Git, and REST API development, specific experience with Azure and its services is lacking.\n",
      "- **Kubernetes, Kafka, Elasticsearch, Spark, and NoSQL databases**: These technologies are mentioned in the job descriptions but are not listed in the candidate's skills or experience.\n",
      "- **JMeter for performance testing**: The second job description specifically requires experience with Apache JMeter, which is not mentioned in the candidate's resume.\n",
      "- **Java and SQL Server**: Although the candidate has experience with SQL, Java is specifically mentioned as a requirement in one of the job descriptions, and SQL Server experience could be beneficial.\n",
      "- **Cloud computing (AWS, GCP, Azure)**: While the candidate has experience with data science and machine learning, broader experience with cloud computing platforms could be advantageous.\n",
      "- **Agile development methodologies**: Although not explicitly mentioned as missing, familiarity with Agile could enhance the candidate's attractiveness for these positions.\n",
      "- **Experience with front-end technologies (HTML/CSS)**: While the candidate has developed a deep learning-based gesture recognition system and has experience with HTML/CSS from an internship, more emphasis on these skills could be beneficial for full-stack development roles.\n",
      "\n",
      "### 2. Recommended Step-by-Step Learning Path\n",
      "\n",
      "To bridge the gap, the candidate could follow this learning path:\n",
      "\n",
      "**Phase 1: Foundations (1-3 months)**\n",
      "\n",
      "1. **Azure and Cloud Computing**:\n",
      "   - Start with Microsoft Azure Fundamentals (AZ-900) to understand Azure services and cloud computing concepts.\n",
      "   - Explore Azure open AI models, AKS, and other relevant services through hands-on labs.\n",
      "\n",
      "2. **Kubernetes and Containerization**:\n",
      "   - Learn Docker to understand containerization.\n",
      "   - Follow up with a Kubernetes course to learn about orchestration.\n",
      "\n",
      "3. **Java Basics**:\n",
      "   - For those without prior Java experience, start with basic Java programming courses.\n",
      "\n",
      "**Phase 2: Specialization (3-6 months)**\n",
      "\n",
      "1. **JMeter and Performance Testing**:\n",
      "   - Take an Apache JMeter course to learn performance testing concepts and how to use JMeter.\n",
      "\n",
      "2. **NoSQL Databases and Big Data Technologies**:\n",
      "   - Learn about MongoDB or another NoSQL database.\n",
      "   - Study Spark, Kafka, and Elasticsearch through online courses or tutorials.\n",
      "\n",
      "3. **SQL Server**:\n",
      "   - If not already familiar, take a course on SQL Server to understand its specific features and how it differs from other SQL databases.\n",
      "\n",
      "**Phase 3: Advanced Topics and Practice (6-12 months)**\n",
      "\n",
      "1. **Agile Methodologies**:\n",
      "   - Study Agile development principles and methodologies.\n",
      "   - Participate in projects that use Agile to gain practical experience.\n",
      "\n",
      "2. **Cloud Computing (AWS, GCP)**:\n",
      "   - Expand knowledge to include AWS and GCP, focusing on services similar to those in Azure.\n",
      "\n",
      "3. **Front-end Development**:\n",
      "   - Enhance HTML/CSS skills and learn JavaScript for front-end development.\n",
      "\n",
      "**Phase 4: Project Application and Certification (Ongoing)**\n",
      "\n",
      "1. **Apply New Skills in Projects**:\n",
      "   - Incorporate new skills into personal or professional projects to gain practical experience.\n",
      "\n",
      "2. **Certifications**:\n",
      "   - Pursue relevant certifications (e.g., Azure Developer Associate, Certified Kubernetes Administrator) to validate skills.\n",
      "\n",
      "### 3. Suggested Resources\n",
      "\n",
      "- **Platforms**:\n",
      "  - Microsoft Learn (for Azure and related technologies)\n",
      "  - Coursera, edX, and Udemy for various courses on cloud computing, Kubernetes, Java, and front-end development\n",
      "  - Kubernetes.io and Docker.com for official tutorials and documentation\n",
      "- **Certifications**:\n",
      "  - Microsoft Certified: Azure Developer Associate\n",
      "  - Certified Kubernetes Administrator (CKA)\n",
      "  - Java Certifications from Oracle\n",
      "- **Communities and Forums**:\n",
      "  - Participate in GitHub, Stack Overflow, and Reddit forums related to the technologies being learned to stay updated and learn from others.\n",
      "- **Books**:\n",
      "  - \"Azure for Architects\" by Ritesh Modi\n",
      "  - \"Kubernetes: Up and Running\" by Brendan Burns and Joe Beda\n",
      "  - \"Java: A Beginner's Guide\" by Herbert Schildt for those new to Java\n",
      "\n",
      "This learning path is tailored to enhance the candidate's technical skills, making them more competitive for the mentioned job roles and similar positions in the industry.\n"
     ]
    }
   ],
   "source": [
    "print(learning_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fdbf6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "def query_groq2(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful resume enhancement assistant that interprets user's resume and enhances them while matching their resumes with suitable jobs and suggesting ways to the user to upskill.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        stop=None,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "707058fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cover_letter(resume_json, selected_job_title, selected_job_description, company_name):\n",
    "    enhance_resume = parse_enhanced_resume(resume_json)\n",
    "    prompt = f\"\"\"\n",
    "        Write a personalized and professional cover letter for the position of \"{selected_job_title}\" at {company_name}.\n",
    "        The letter should be 3-4 paragraphs, tailored to the job description below, and should highlight how the candidate's skills align with the company's requirements.\n",
    "\n",
    "        --- Candidate's Resume ---\n",
    "        {enhance_resume}\n",
    "\n",
    "        --- Job Description ---\n",
    "        {selected_job_description}\n",
    "\n",
    "        Ensure the tone is confident, enthusiastic, and formal. Avoid generic phrases. Mention specific skills or experiences from the resume that match the job. End with a call to action and interest in an interview.\n",
    "        Begin your response directly from the actual response, no need to give headers like 'Here is your generated cover letter'.\n",
    "    \"\"\"\n",
    "\n",
    "    return query_groq2(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a7f389f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cover letter to a .txt file\n",
    "def save_cover_letter_to_txt(cover_letter_text, filename=\"cover_letter.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cover_letter_text)\n",
    "    print(f\"Cover letter saved as '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "778fd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_jobs = match_jobs(resume_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bb167d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_jobs = json.loads(matched_jobs)\n",
    "\n",
    "#Simulate user selecting a job\n",
    "selected_index = 0  \n",
    "selected_job = matched_jobs[\"matched_jobs\"][selected_index]\n",
    "\n",
    "selected_title = selected_job[\"title\"]\n",
    "selected_company = selected_job[\"company_name\"]\n",
    "selected_description = selected_job[\"description\"]\n",
    "\n",
    "# Generate the cover letter\n",
    "cover_letter = generate_cover_letter(\n",
    "    resume_json=resume_json,\n",
    "    selected_job_title=selected_title,\n",
    "    selected_job_description=selected_description,\n",
    "    company_name=selected_company\n",
    ")\n",
    "\n",
    "# 5. Save the cover letter to a file\n",
    "def save_cover_letter_to_txt(cover_letter, filename=\"cover_letter.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cover_letter)\n",
    "\n",
    "save_cover_letter_to_txt(cover_letter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
