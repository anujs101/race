# -*- coding: utf-8 -*-
"""enhancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xm9HGhrVsXCBL48wF3DQYI-KGiRX6Aax
"""

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle
import re
from tqdm.notebook import tqdm

model = SentenceTransformer("all-MiniLM-L6-v2")

ats_snippets = [
    "Use action verbs like 'Led', 'Managed', 'Developed', instead of passive phrases.",
    "Quantify your achievements, e.g., 'increased sales by 20%'.",
    "Keep resume length to one page unless you have 10+ years of experience.",
    "Tailor your resume to each job description by including relevant keywords.",
    "Use consistent formatting: bullet points, font size, spacing.",
    "Avoid vague terms like 'team player', focus on specific results.",
    "List technical skills and tools separately in a skills section.",
    "Start each bullet point with a powerful verb."
]


cv_guide_text = """
Every graduate student needs a curriculum vitae, or CV
Your CV represents your accomplishments and experience as an academic and helps to establish your
professional image.  Well before you apply for faculty positions, you will use your CV to apply for
fellowships and grants, to accompany submissions for publications or conference papers, when being
considered for leadership roles or consulting projects, and more.  CV's are also used when applying for
some positions outside academia, such as in think tanks or research institutes, or for research positions in
industry.
As you progress through graduate school, you will, of course, add to your CV, but the basic areas to
include are your contact information, education, research experience, teaching experience, publications,
presentations, honors and awards, and contact information for your references, or those people willing to
speak or write on your behalf.
Some formatting pointers:
 There is no single best format. Refer to samples for ideas, but craft your CV to best reflect you
 and your unique accomplishments.
  Unlike a resume, there is no page limit, but most graduate student's CVs are two to five pages in
 length.  Your CV may get no more than thirty seconds of the reader's attention, so ensure the
 most important information stands out. Keep it concise and relevant!
  Be strategic in how you order and entitle your categories.  The most important information
 should be on the first page.  Within each category, list items in reverse chronological order.
 Category headings influence how readers perceive you. For example, the same experience could
 belong in a category entitled: “Service to the Field,” “Conferences Organized,” or “Relevant
 Professional Experience.”
  Use active verbs and sentence fragments (not full sentences) to describe your experiences. Avoid
 pronouns (e.g. I, me), and minimize articles (a, and, the). Use a level of jargon most appropriate
 for your audience. Keep locations, dates and less important information on the right side of the
 page the left side should have important details like university, degree, job title, etc.
  Stick to a common font, such as Times New Roman, using a font size of 10 to 12 point. Use
 highlighting judiciously, favoring bold, ALL CAPS, and white space to create a crisp
 professional style.  Avoid text boxes, underlining, and shading; italics may be used in
 moderation. Margins should be equal on all four sides, and be ¾ to 1 inch in size.
  And most importantly…Follow the conventions of your field!  Different academic disciplines
 have different standards and expectations, especially in the order of categories.  Check out CVs
 from recent graduates of your department, and others in your field, to ensure you are following
 your field's norms.
 Tailor your CV to the position, purpose, or audience
“Why should we select YOU?” - That is the question on the top of your reader's mind, so craft your CV
to convince the reader that you have the skills, experience, and knowledge they seek. Depending on the
purpose, you might place more or less emphasis on your teaching experience, for example. Also, keep
an archival CV (for your eyes only!) that lists all the details of everything you've done - tailor from
there.
"""

chunks = [chunk.strip() for chunk in re.split(r'\n\s*\n', cv_guide_text) if chunk.strip()]

embeddings = model.encode(chunks)
embeddings = np.array(embeddings).astype('float32')

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

faiss.write_index(index, "cv_guide.index")
with open("cv_guide_texts.pkl", "wb") as f:
    pickle.dump(chunks, f)

# from sentence_transformers import SentenceTransformer
# import faiss
# import numpy as np
# import pickle

# model = SentenceTransformer('all-MiniLM-L6-v2')

# embeddings = model.encode(ats_snippets)

# embeddings = np.array(embeddings).astype('float32')

# index = faiss.IndexFlatL2(embeddings.shape[1])
# index.add(embeddings)

# faiss.write_index(index, "ats_guidelines.index")
# with open("ats_texts.pkl", "wb") as f:
#     pickle.dump(ats_snippets, f)

def retrieve_cv_guidelines(query_text, top_k=3):
    query_embedding = model.encode([query_text]).astype("float32")
    index = faiss.read_index("cv_guide.index")
    with open("cv_guide_texts.pkl", "rb") as f:
        guide_chunks = pickle.load(f)

    distances, indices = index.search(query_embedding, top_k)
    return [guide_chunks[i] for i in indices[0]]

resume_json = {
    "status": "success",
    "message": "Image-based resume processed using AI Vision",
    "data": {
        "resumeId": "6804307e8ed758f47071b131",
        "classification": {
            "contactInfo": {
                "name": "SHAH NAMAN CHETAN",
                "email": "namanshah434@gmail.com",
                "phone": "9372576225",
                "address": "Mumbai",
                "linkedin": "https://in.linkedin.com/in/naman-shah-5478232bb"
            },
            "education": [
                "B.Tech in Computer Science and Engineering\nBharatiya Vidya Bhavan's Sardar Patel Institute of Technology (SPIT)\n2023 - Present  Mumbai, India",
                "BS Degree in Data Science\nIndian Institute of Technology (IIT) Madras\n05/2023 - Present  Online"
            ],
            "experience": [],
            "projects": [
                {
                    "name": "RGB Distorted QR Code",
                    "description": "This project involved creating a QR code generation system that incorporates RGB distortion and encryption for increased security. Developed a system that generates unscannable QR codes by distorting RGB values and encrypting them with a key-based decryption mechanism."
                }
            ],
            "skills": [
                "CSS",
                "Encryption",
                "HTML",
                "Java",
                "Matplotlib",
                "Numpy",
                "Pandas",
                "Python",
                "React",
                "Scikit-Learn",
                "Gmail"
            ],
            "certifications": [
                "Foundations of Programming and Data Science\nCompleted Foundations of Programming and Data Science, IIT Madras"
            ],
            "achievements": [
                "Semi-finalist at Build With India Hackathon, a national-level competition",
                "Winner of Unsolved, organized by CSI SPIT"
            ]
        },
        "isScannedDocument": True
    }
}

def keyword_match_score(resume_text, job_text):
    resume_words = set(resume_text.lower().split())
    job_words = set(job_text.lower().split())
    overlap = resume_words & job_words
    return len(overlap) / len(job_words)  # normalized score

from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("all-MiniLM-L6-v2")

def skill_similarity(resume_skills, job_skills):
    res_emb = model.encode(resume_skills, convert_to_tensor=True)
    job_emb = model.encode(job_skills, convert_to_tensor=True)
    return util.pytorch_cos_sim(res_emb, job_emb).item()

def has_required_sections(resume_text):
    sections = ["experience", "education", "skills", "projects", "certifications"]
    return sum(1 for s in sections if s in resume_text.lower())

def ats_score(resume_text, job_text):
    keyword_score = keyword_match_score(resume_text, job_text)
    skill_score = skill_similarity(resume_text, job_text)
    section_score = has_required_sections(resume_text) / 5
    return round((0.4 * keyword_score + 0.4 * skill_score + 0.2 * section_score) * 100, 2)

def flatten_resume_json(resume_json):
    classification = resume_json["data"]["classification"]
    parts = []

    # Contact Info
    contact = classification.get("contactInfo", {})
    parts.append(f"Name: {contact.get('name', '')}")
    parts.append(f"Email: {contact.get('email', '')}")
    parts.append(f"Phone: {contact.get('phone', '')}")
    parts.append(f"Address: {contact.get('address', '')}")
    parts.append(f"LinkedIn: {contact.get('linkedin', '')}")

    # Education
    education = classification.get("education", [])
    if education:
        parts.append("\nEducation:")
        for edu in education:
            parts.append(f"- {edu}")

    # Experience
    experience = classification.get("experience", [])
    if experience:
        parts.append("\nExperience:")
        for exp in experience:
            parts.append(f"- {exp['role']} at {exp['organization']} ({exp['duration']}): {exp['description']}")

    # Projects
    projects = classification.get("projects", [])
    if projects:
        parts.append("\nProjects:")
        for proj in projects:
            parts.append(f"- {proj['name']}: {proj['description']}")

    # Skills
    skills = classification.get("skills", [])
    if skills:
        parts.append("\nSkills: " + ", ".join(skills))

    # Certifications
    certs = classification.get("certifications", [])
    if certs:
        parts.append("\nCertifications:")
        for cert in certs:
            parts.append(f"- {cert}")

    # Achievements
    achievements = classification.get("achievements", [])
    if achievements:
        parts.append("\nAchievements:")
        for ach in achievements:
            parts.append(f"- {ach}")

    return "\n".join(parts)

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')

def embed_resume_for_future_matching(resume_text):
    emb = model.encode([resume_text]).astype("float32")
    index = faiss.IndexFlatL2(emb.shape[1])
    index.add(emb)
    faiss.write_index(index, "resume_vectors.index")

resume_text = flatten_resume_json(resume_json)
rag_context = retrieve_cv_guidelines(resume_text, top_k=3)

def build_prompt(resume_text, rag_context, user_instruction=""):
    return f"""
You are a resume enhancement AI.

From the following raw resume data and RAG context, extract and rewrite content into structured professional resume sections: About, Skills, Experience, Education, Projects, Certifications, and Achievements.

In the Skills section, group skills under appropriate categories such as:
- Programming Languages
- Frameworks & Libraries
- Tools & Platforms
- Databases
- Cloud Services
- Soft Skills

{user_instruction}

Only return the enhanced resume content. Content should fit into a single page. Do NOT include any explanations, notes, or repeat the prompt.

=== RAG CONTEXT ===
{rag_context}
=== Resume Input ===
{resume_text}

=== Enhanced Resume ===
"""

import re

def interpret_user_instruction(raw_input):
    instruction = raw_input.strip().lower()

    if "explain" in instruction:
        return "Please explain the strengths and weaknesses of the candidate based on the provided resume."

    match_add = re.search(r"add (.+?) to my resume", instruction)
    if match_add:
        fields = match_add.group(1).strip()
        return f"Add the following information to the resume: {fields}"

    match_keep_projects = re.search(r"keep (\d+) projects", instruction)
    if match_keep_projects:
        count = match_keep_projects.group(1)
        return f"Limit the number of projects in the resume to {count}, selecting the most impressive ones from both existing and newly provided content."

    return instruction  # fallback for custom or unrecognized instructions

import requests

GROQ_API_KEY = "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"

def query_groq1(prompt, model="llama3-8b-8192"):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a resume enhancer AI. Output structured resume sections only."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 1024
    }

    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    return response.json()["choices"][0]["message"]["content"]

# raw_user_input = "add AWS and GCP cloud experience to my resume"

# prompt = build_prompt(resume_text, rag_context, user_instruction)

# enhanced_resume = query_groq1(prompt)

def parse_enhanced_resume(raw_text):
    import re

    # Define section headings
    section_titles = [
        "About", "Skills", "Experience", "Education", "Projects",
        "Certifications", "Achievements"
    ]

    sections = {title.lower(): [] for title in section_titles}
    current_section = None

    lines = raw_text.strip().splitlines()
    for line in lines:
        # Detect section header
        match = re.match(r"\*\*(.*?)\*\*", line.strip())
        if match:
            header = match.group(1).strip()
            if header in section_titles:
                current_section = header.lower()
                continue

        # Store lines under the current section
        if current_section:
            content = line.strip("•").strip("-").strip()
            if content:
                sections[current_section].append(content)

    return sections

metadata = resume_json["data"]["classification"]["contactInfo"]

def get_parsed_resume(resume_text, rag_context, user_instruction=""):
    prompt = build_prompt(resume_text, rag_context, user_instruction)
    enhanced_resume = query_groq1(prompt)
    parsed_data = parse_enhanced_resume(enhanced_resume)
    return parsed_data

# parsed_data = parse_enhanced_resume(enhanced_resume)
# resume_data = {
#     "name": metadata.get("name", ""),
#     "email": metadata.get("email", ""),
#     "phone": metadata.get("phone", ""),
#     "address": metadata.get("address", ""),
#     "linkedin": metadata.get("linkedin", ""),
#     "about": " ".join(parsed_data.get("about", [])) or "N/A",
#     "skills": parsed_data.get("skills", []),
#     "experience": [],
#     "education": [],
#     "projects": [],
#     "certifications": parsed_data.get("certifications", []),
#     "achievements": parsed_data.get("achievements", []),
# }

# Define user_instruction somewhere at the top, or pass it dynamically
user_instruction = user_instruction if 'user_instruction' in locals() else ""

# Then safely call get_parsed_resume
try:
    parsed_resume = get_parsed_resume(resume_text, rag_context, user_instruction)
except Exception as e:
    print("❌ Error in get_parsed_resume:", e)
    parsed_resume = {}

# Now construct resume_data safely
resume_data = {
    "name": metadata.get("name", ""),
    "email": metadata.get("email", ""),
    "phone": metadata.get("phone", ""),
    "address": metadata.get("address", ""),
    "linkedin": metadata.get("linkedin", ""),
    "about": " ".join(parsed_resume.get("about", [])) or "N/A",
    # Add more fields as needed
}


resume_data = {
    "name": metadata.get("name", ""),
    "email": metadata.get("email", ""),
    "phone": metadata.get("phone", ""),
    "address": metadata.get("address", ""),
    "linkedin": metadata.get("linkedin", ""),
    "about": " ".join(get_parsed_resume(resume_text, rag_context, user_instruction).get("about", [])) or "N/A",
    "skills": parsed_resume.get("skills", []),
    "experience": parsed_resume.get("experience", []),
    "education": parsed_resume.get("education", []),
    "projects": parsed_resume.get("projects", []),
    "certifications": parsed_resume.get("certifications", []),
    "achievements": parsed_resume.get("achievements", []),
}


from jinja2 import Environment, FileSystemLoader

def render_latex(resume_data, template_path="resume_template.tex"):
    env = Environment(loader=FileSystemLoader('.'))
    template = env.get_template(template_path)
    return template.render(resume_data)

latex_code = render_latex(resume_data)

with open("resume_output.tex", "w", encoding="utf-8") as f:
    f.write(latex_code)

GROQ_API_KEY = "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"

def query_groq2(prompt, model="llama3-8b-8192"):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a resume enhancer AI. Output structured resume sections only."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 1024
    }

    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    return response.json()["choices"][0]["message"]["content"]

import time
import serpapi
import json

def get_multiple_jobs_with_pagination(job_title, location):
    params = {
        "engine": "google_jobs",
        "q": job_title,
        "location": location,
        "api_key": '83c1ef3c99b32b05ab29da61937948e1cce626b355feb3c4c6ead197a08a7aac',
        "hl": "en",
        "gl": "in"
    }
    max_jobs = 5
    all_jobs = []
    next_page_token = None

    while len(all_jobs) < max_jobs:
        if next_page_token:
            params["next_page_token"] = next_page_token
        else:
            params.pop("next_page_token", None)

        search = serpapi.search(params)   # returns SerpResults (dict-like)
        data = search

        jobs = data.get("jobs_results", [])
        all_jobs.extend(jobs)

        # Pagination
        serpapi_pagination = data.get("serpapi_pagination", {})
        next_page_token = serpapi_pagination.get("next_page_token")

        if not next_page_token:
            break

        time.sleep(1)

    all_jobs = all_jobs[:max_jobs]

    result = {}
    for idx, job in enumerate(all_jobs, 1):
        description = job.get('description', '')
        company_name = job.get('company_name', '')
        application_link = ""
        if 'apply_options' in job and job['apply_options']:
            application_link = job['apply_options'][0].get('link', '')
        elif 'via' in job:
            application_link = job['via']
        else:
            application_link = job.get('detected_extensions', {}).get('apply_link', '')

        actual_job_title = job.get('title', f"{job_title} Opportunity {idx}")
        result[actual_job_title] = {
            "company_name": company_name,
            "description": description,
            "application_link": application_link
        }

    return result

job_title = "Fullstack Developer"
location = "India"
job_descriptions_json = get_multiple_jobs_with_pagination(job_title, location)

# Step 2: Load embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Step 3: Extract descriptions and metadata
descriptions = []
metadata = []

for title, data in job_descriptions_json.items():
    descriptions.append(data["description"])
    metadata.append({
        "title": title,
        "company_name": data["company_name"],
        "application_link": data["application_link"]
    })

# Step 4: Generate embeddings
embeddings = model.encode(descriptions)
embeddings_np = np.array(embeddings).astype("float32")  # FAISS requires float32

# Step 5: Create FAISS index and add embeddings
dimension = embeddings_np.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings_np)

# Optional: Save FAISS index
faiss.write_index(index, "job_faiss.index")

# Step 6: Save metadata for lookup
with open("job_faiss_metadata.json", "w") as f:
    json.dump(metadata, f, indent=2)

index = faiss.read_index("job_faiss.index")
with open("job_faiss_metadata.json", "r") as f:
    metadata = json.load(f)

# Load embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

user_resume_text = enhanced_resume

resume_embedding = model.encode([user_resume_text]).astype("float32")

top_k = 3
D, I = index.search(resume_embedding, top_k)

print("Top Matching Jobs:\n")
for idx in I[0]:
    job = metadata[idx]
    print(f"{job['title']} at {job['company_name']}\nLink: {job['application_link']}\n")

job_desc_text = "\n\n".join([metadata[i]["title"] + ":\n" + descriptions[i] for i in I[0]])

rag_prompt = f"""
You are a career advisor AI. The following is a candidate's resume:

--- RESUME ---
{user_resume_text}

These are the job descriptions of top matches:

--- JOB DESCRIPTIONS ---
{job_desc_text}

1. Identify what technical or domain-specific skills the candidate is missing.
2. Recommend a step-by-step learning path (with topics/tools/technologies) to bridge the gap.
3. Suggest resources (platforms or certifications) for each skill if possible.
"""

learning_path = query_groq2(rag_prompt)

GROQ_API_KEY = "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"

def query_groq3(prompt, model="llama3-8b-8192"):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a resume enhancer AI. Output structured resume sections only."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 1024
    }

    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    return response.json()["choices"][0]["message"]["content"]

def generate_cover_letter(user_resume, selected_job_title, selected_job_description, company_name):
    prompt = f"""
Write a personalized and professional cover letter for the position of "{selected_job_title}" at {company_name}.
The letter should be 3-4 paragraphs, tailored to the job description below, and should highlight how the candidate's skills align with the company's requirements.

--- Candidate's Resume ---
{user_resume}

--- Job Description ---
{selected_job_description}

Ensure the tone is confident, enthusiastic, and formal. Avoid generic phrases. Mention specific skills or experiences from the resume that match the job. End with a call to action and interest in an interview.
Begin your response directly from the actual response, no need to give headers like 'Here is your generated cover letter'.
"""

    return query_groq3(prompt, model="llama3-8b-8192")

# Save the cover letter to a .txt file
def save_cover_letter_to_txt(cover_letter_text, filename="cover_letter.txt"):
    with open(filename, "w", encoding="utf-8") as f:
        f.write(cover_letter_text)
    print(f"Cover letter saved as '{filename}'")

# Let's say the user selects job index 1 (second from search results)
selected_index = I[0][1]
selected_job = metadata[selected_index]

selected_title = selected_job.get("title", "Fullstack Developer")  # fallback
selected_description = descriptions[selected_index]
selected_company = selected_job["company_name"]

score = ats_score(enhanced_resume, selected_description)
print(score)

cover_letter = generate_cover_letter(
    user_resume=user_resume_text,
    selected_job_title=selected_title,
    selected_job_description=selected_description,
    company_name=selected_company
)

save_cover_letter_to_txt(cover_letter)

"""Explainable AI"""

import matplotlib.pyplot as plt
import networkx as nx

def draw_race_workflow():
    G = nx.DiGraph()

    nodes = [
        "Upload Resume (PDF)",
        "Parse & Extract Resume JSON",
        "Embed Resume using MiniLM",
        "Enhance Resume with RAG + LLM",
        "Generate ATS-optimized LaTeX Resume",
        "Scrape & Embed Job Descriptions",
        "Match Top Jobs via FAISS Similarity",
        "Identify Skill Gaps (LLM)",
        "Generate Upskilling Roadmap",
        "Build Personalized Cover Letters",
        "Frontend Visualizer + Job Dashboard"
    ]

    edges = [
        ("Upload Resume (PDF)", "Parse & Extract Resume JSON"),
        ("Parse & Extract Resume JSON", "Embed Resume using MiniLM"),
        ("Embed Resume using MiniLM", "Enhance Resume with RAG + LLM"),
        ("Enhance Resume with RAG + LLM", "Generate ATS-optimized LaTeX Resume"),
        ("Embed Resume using MiniLM", "Match Top Jobs via FAISS Similarity"),
        ("Scrape & Embed Job Descriptions", "Match Top Jobs via FAISS Similarity"),
        ("Match Top Jobs via FAISS Similarity", "Identify Skill Gaps (LLM)"),
        ("Identify Skill Gaps (LLM)", "Generate Upskilling Roadmap"),
        ("Match Top Jobs via FAISS Similarity", "Build Personalized Cover Letters"),
        ("Generate ATS-optimized LaTeX Resume", "Frontend Visualizer + Job Dashboard"),
        ("Generate Upskilling Roadmap", "Frontend Visualizer + Job Dashboard"),
        ("Build Personalized Cover Letters", "Frontend Visualizer + Job Dashboard")
    ]

    G.add_nodes_from(nodes)
    G.add_edges_from(edges)

    pos = nx.spring_layout(G, seed=42)

    plt.figure(figsize=(18, 12))
    nx.draw_networkx_nodes(G, pos, node_color='lightcoral', node_size=3000)
    nx.draw_networkx_edges(G, pos, arrows=True, arrowsize=25)
    nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold')

    plt.title("RACE Platform Workflow - Explainable AI", fontsize=18)
    plt.axis("off")
    plt.tight_layout()
    plt.show()

draw_race_workflow()